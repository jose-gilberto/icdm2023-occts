{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any, Tuple, Optional\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, out_channels: int, out_lenght: int) -> None:\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.out_lenght = out_lenght\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), self.out_channels, self.out_lenght)\n",
    "    \n",
    "\n",
    "class Cholesky(torch.autograd.Function):\n",
    "    def forward(ctx, a):\n",
    "        l = torch.cholesky(a, False)\n",
    "        ctx.save_for_backward(l)\n",
    "        return l\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        l, = ctx.saved_variables\n",
    "        linv = l.inverse()\n",
    "        inner = torch.tril(torch.mm(l.t(), grad_output)) * torch.tril(\n",
    "            1.0 - Variable(l.data.new(l.size(1)).fill_(0.5).diag()))\n",
    "        s = torch.mm(linv.t(), torch.mm(inner, linv))\n",
    "        return s\n",
    "    \n",
    "\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model: nn.Module, lambda_energy, lambda_cov, device: torch.device, n_gmm):\n",
    "        self.model = model\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov = lambda_cov\n",
    "        self.device = device\n",
    "        self.n_gmm = n_gmm\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, x_hat: torch.Tensor, z: torch.Tensor, gamma: torch.Tensor) -> torch.Tensor:\n",
    "        reconstruction_loss = torch.mean((x - x_hat).pow(2))\n",
    "\n",
    "        sample_energy, cov_diag = self.compute_energy(z, gamma)\n",
    "        \n",
    "        loss = reconstruction_loss + self.lambda_energy * sample_energy + self.lambda_cov * cov_diag\n",
    "        return Variable(loss, requires_grad=True)\n",
    "\n",
    "    def compute_energy(self, z: torch.Tensor, gamma: torch.Tensor, phi = None, mu = None, cov = None, sample_mean = True) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if (phi is None) or (mu is None) or (cov is None):\n",
    "            phi, mu, cov = self.compute_params(z, gamma)\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "        \n",
    "        eps = 1e-12\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        for k in range(self.n_gmm):\n",
    "            cov_k = cov[k] + (torch.eye(cov[k].size(-1)) * eps).to(self.device)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "            det_cov.append((Cholesky.apply(cov_k.cpu() * (2 * np.pi)).diag().prod()).unsqueeze(0))\n",
    "            cov_diag += torch.sum(1 / cov_k.diag())\n",
    "            \n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        det_cov = torch.cat(det_cov).to(self.device)\n",
    "        \n",
    "        E_z = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        E_z = torch.exp(E_z)\n",
    "        E_z = -torch.log(torch.sum(phi.unsqueeze(0) * E_z / (torch.sqrt(det_cov)).unsqueeze(0), dim=1) + eps)\n",
    "        if sample_mean == True:\n",
    "            E_z = torch.mean(E_z)\n",
    "        return E_z, cov_diag\n",
    "            \n",
    "    def compute_params(self, z: torch.Tensor, gamma: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        phi = torch.sum(gamma, dim=0) / gamma.size(0)\n",
    "        \n",
    "        mu = torch.sum(z.unsqueeze(1) * gamma.unsqueeze(-1), dim=0)\n",
    "        mu /= torch.sum(gamma, dim=0).unsqueeze(-1)\n",
    "        \n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "        z_mu_z_mu_t = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "        \n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_z_mu_t, dim=0)\n",
    "        cov /= torch.sum(gamma, dim=0).unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        return phi, mu, cov\n",
    "\n",
    "\n",
    "class DAGMM(pl.LightningModule):\n",
    "    def __init__(self, sequence_length: int, num_features: int = 1, n_gmm: int = 2, z_dim: int = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=sequence_length * num_features,\n",
    "                      out_features=60),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=60, out_features=30),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=30, out_features=10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=10, out_features=z_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=z_dim, out_features=10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=10, out_features=30),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=30, out_features=60),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=60, out_features=sequence_length*num_features),\n",
    "            Upscale(out_channels=num_features, out_lenght=sequence_length)\n",
    "        )\n",
    "\n",
    "        self.estimation_net = nn.Sequential(\n",
    "            nn.Linear(in_features=z_dim + 2, out_features=10),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=10, out_features=n_gmm),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.criteria = ComputeLoss(self, 0.1, 0.005, self.device, 4)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encode(x)\n",
    "        \n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def estimate(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.estimation_net(z)\n",
    "    \n",
    "    def compute_reconstruction(self, x: torch.Tensor, x_hat: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        relative_euclidean_distance = (x - x_hat).norm(2, dim=1) / x.norm(2, dim=1)\n",
    "        cosine_similarity = F.cosine_similarity(x, x_hat, dim=1)\n",
    "        return relative_euclidean_distance, cosine_similarity\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z_c = self.encode(x)\n",
    "        x_hat = self.decode(z_c)\n",
    "        rec_1, rec_2 = self.compute_reconstruction(x, x_hat)\n",
    "        z = torch.cat([z_c, rec_1.unsqueeze(-1), rec_2.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimate(z)\n",
    "        return z_c, x_hat, z, gamma\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx) -> torch.Tensor:\n",
    "        self.criteria.device = self.device\n",
    "        \n",
    "        x, _ = batch\n",
    "        \n",
    "        _, x_hat, z, gamma = self(x)\n",
    "        loss = self.criteria.forward(x, x_hat, z, gamma)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def evaluate(self, train_loader: DataLoader, test_loader: DataLoader, n_gmm) -> None:\n",
    "        self.eval()\n",
    "        compute = ComputeLoss(self, None, None, self.device, n_gmm)\n",
    "        with torch.no_grad:\n",
    "\n",
    "            n_samples = 0\n",
    "            gamma_sum = 0\n",
    "            mu_sum = 0\n",
    "            cov_sum = 0\n",
    "            \n",
    "            for x, _ in train_loader:\n",
    "                x = x.float().to(self.device)\n",
    "                \n",
    "                _, _, z, gamma = self(x)\n",
    "                phi_batch, mu_batch, cov_batch = compute.compute_params(z, gamma)\n",
    "                \n",
    "                batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "                gamma_sum += batch_gamma_sum\n",
    "                mu_sum += mu_batch * batch_gamma_sum.unsqueeze(-1)\n",
    "                cov_sum += cov_batch * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "                \n",
    "                n_samples += x.size(0)\n",
    "                \n",
    "            train_phi = gamma_sum / n_samples\n",
    "            train_mu = mu_sum / n_samples\n",
    "            train_cov = cov_sum / n_samples\n",
    "            \n",
    "            energy_train = []\n",
    "            labels_train = []\n",
    "            \n",
    "            for x, y in train_loader:\n",
    "                x = x.float().to(self.device)\n",
    "                \n",
    "                _, _, z, gamma = self(x)\n",
    "                sample_energy, cov_diag = compute.compute_energy(z, gamma, phi=train_phi, mu=train_mu, cov=train_cov, sample_mean=False)\n",
    "                energy_train.append(sample_energy.detach().cpu())\n",
    "                labels_train.append(y)\n",
    "                \n",
    "            energy_train = torch.cat(energy_train).numpy()\n",
    "            labels_train = torch.cat(labels_train).numpy()\n",
    "            \n",
    "            energy_test = []\n",
    "            labels_test = []\n",
    "            \n",
    "            for x, y in test_loader:\n",
    "                x = x.float().to(self.device)\n",
    "                _, _, z, gamma = self(x)\n",
    "                sample_energy, cov_diag = compute.compute_energy(z, gamma, train_phi, train_mu, train_cov, sample_mean=False)\n",
    "                energy_test.append(sample_energy.detach().cpu())\n",
    "                labels_test.append(y)\n",
    "                \n",
    "            energy_test = torch.cat(energy_test).numpy()\n",
    "            labels_test = torch.cat(labels_test).numpy()\n",
    "            \n",
    "            scores_total = np.concatenate((energy_train, energy_test), axis=0)\n",
    "            labels_total = np.concatenate((labels_train, labels_test), axis=0)\n",
    "            \n",
    "        threshold = np.percentile(scores_total, 100-20)\n",
    "        pred = (energy_test > threshold).astype(int)\n",
    "        gt = labels_test.astype(int)\n",
    "        accuracy = accuracy(gt, pred)\n",
    "        precision, recall, f1, _ = prf(gt, pred, average='binary')\n",
    "        \n",
    "        return accuracy, precision, recall, f1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
