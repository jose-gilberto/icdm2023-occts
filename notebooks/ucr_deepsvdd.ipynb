{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=-1)\n",
    "    \n",
    "\n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, out_channels: int, out_lenght: int) -> None:\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.out_lenght = out_lenght\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), self.out_channels, self.out_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union, Tuple\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x, y) -> None:\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        assert len(self.x) == len(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return torch.from_numpy(self.x[index]).float(), torch.from_numpy(np.array([self.y[index]]))\n",
    "\n",
    "\n",
    "class DeepSVDDAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, sequence_length: int, in_channels: int, representation_dim: int = 32) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.representation_dim = representation_dim\n",
    "\n",
    "        # --- Encoder --- #\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            nn.Conv1d(in_channels=self.in_channels, out_channels=128, kernel_size=7, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            GlobalAveragePooling(),\n",
    "            nn.Linear(in_features=128, out_features=self.representation_dim)\n",
    "        ])\n",
    "        \n",
    "        # --- Decoder --- #\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            nn.Linear(in_features=self.representation_dim, out_features=128 * self.sequence_length),\n",
    "            Upscale(out_lenght=self.sequence_length, out_channels=128),\n",
    "            nn.ConvTranspose1d(in_channels=128, out_channels=128, kernel_size=3, bias=False, stride=1, dilation=1, padding=3//2),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.ConvTranspose1d(in_channels=128, out_channels=256, kernel_size=3, bias=False, stride=1, dilation=1, padding=3//2),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.ConvTranspose1d(in_channels=256, out_channels=128, kernel_size=5, bias=False, stride=1, dilation=1, padding=5//2),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.ConvTranspose1d(in_channels=128, out_channels=1, kernel_size=7, bias=False, stride=1, dilation=1, padding=7//2),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n",
    "    \n",
    "    def configure_optimizers(self) -> Any:\n",
    "        # Set optimizer for the autoencoder task\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2, weight_decay=1e-6, amsgrad=False)\n",
    "        # Set learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[250], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        x_hat, z = self(x)\n",
    "        \n",
    "        loss = torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim())))\n",
    "        loss = torch.mean(loss)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> List[Dict[str, Any]]:\n",
    "        x, y = batch\n",
    "        x_hat, z = self(x)\n",
    "\n",
    "        loss = torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim())))\n",
    "        loss = torch.mean(loss)\n",
    "        \n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "class DeepSVDD(pl.LightningModule):\n",
    "    def __init__(self, sequence_length: int, in_channels: int, representation_dim: int = 32) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.in_channels = in_channels\n",
    "        self.representation_dim = representation_dim\n",
    "        \n",
    "        self.R = torch.tensor(0.0, device=self.device)\n",
    "        self.nu = 0.1\n",
    "        self.center = None\n",
    "        \n",
    "        self.encoder = nn.Sequential(*[\n",
    "            nn.Conv1d(in_channels=1, out_channels=128, kernel_size=7, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, bias=False, stride=1, dilation=1, padding='same'),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "            GlobalAveragePooling(),\n",
    "            nn.Linear(in_features=128, out_features=32)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def init_center(self, loader: DataLoader, eps: Optional[float] = 0.01) -> torch.Tensor:\n",
    "        n_samples = 0\n",
    "        center = torch.zeros(self.representation_dim, device=self.device)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for (x, y) in loader:\n",
    "                x = x.to(self.device)\n",
    "                z = self(x)\n",
    "\n",
    "                n_samples += z.shape[0]\n",
    "                center += torch.sum(z, dim=0)\n",
    "\n",
    "        center /= n_samples\n",
    "\n",
    "        center[(abs(center) < eps) & (center < 0)] = -eps\n",
    "        center[(abs(center) < eps) & (center > 0)] = eps\n",
    "        \n",
    "        return center\n",
    "    \n",
    "    def get_radius(self, distance: torch.Tensor, nu: float):\n",
    "        return np.quantile(np.sqrt(distance.clone().data.cpu().numpy()), 1 - nu)\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5, weight_decay=1e-6, amsgrad=False)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150], gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        x, y = batch\n",
    "        z = self(x)\n",
    "        \n",
    "        distance = torch.sum((z - self.center) ** 2, dim=1)\n",
    "        scores = distance - self.R ** 2\n",
    "        loss = self.R ** 2 + (1 / self.nu) * torch.mean(torch.max(torch.zeros_like(scores), scores))\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        if self.current_epoch >= 10:\n",
    "            self.R.data = torch.tensor(self.get_radius(distance, self.nu), device=self.device)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> List[Dict[str, Any]]:\n",
    "        x, y = batch\n",
    "        z = self(x)\n",
    "\n",
    "        distance = torch.sum((z - self.center) ** 2, dim=1)\n",
    "        scores = distance - self.R ** 2\n",
    "\n",
    "        preds = torch.max(torch.zeros_like(scores), scores).tolist()\n",
    "        preds = np.array([1 if pred > 0 else -1 for pred in preds])\n",
    "\n",
    "        self.log('accuracy_score', accuracy_score(preds, y.cpu().numpy()))\n",
    "        self.log('f1', f1_score(preds, y.cpu().numpy()))\n",
    "        self.log('recall', recall_score(preds, y.cpu().numpy()))\n",
    "        self.log('precision', precision_score(preds, y.cpu().numpy()))\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCR_DATASETS = [\n",
    "    'Adiac',\n",
    "    'ArrowHead',\n",
    "    'Beef',\n",
    "    'BeetleFly',\n",
    "    'BirdChicken',\n",
    "    'Car',\n",
    "    'CBF',\n",
    "    'ChlorineConcentration',\n",
    "    'CinCECGTorso',\n",
    "    'Coffee',\n",
    "    'Computers',\n",
    "    'CricketX',\n",
    "    'CricketY',\n",
    "    'CricketZ',\n",
    "    'DiatomSizeReduction',\n",
    "    'DistalPhalanxOutlineAgeGroup',\n",
    "    'DistalPhalanxOutlineCorrect',\n",
    "    'DistalPhalanxTW',\n",
    "    'Earthquakes',\n",
    "    'ECG200',\n",
    "    'ECG5000',\n",
    "    'ECGFiveDays',\n",
    "    'ElectricDevices',\n",
    "    'FaceAll',\n",
    "    'FaceFour',\n",
    "    'FacesUCR',\n",
    "    'FiftyWords',\n",
    "    'Fish',\n",
    "    'FordA',\n",
    "    'FordB',\n",
    "    'GunPoint',\n",
    "    'Ham',\n",
    "    'HandOutlines',\n",
    "    'Haptics',\n",
    "    'Herring',\n",
    "    'InlineSkate',\n",
    "    'InsectWingbeatSound',\n",
    "    'ItalyPowerDemand',\n",
    "    'LargeKitchenAppliances',\n",
    "    'Lightning2',\n",
    "    'Lightning7',\n",
    "    'Mallat',\n",
    "    'Meat',\n",
    "    'MedicalImages',\n",
    "    'MiddlePhalanxOutlineAgeGroup',\n",
    "    'MiddlePhalanxOutlineCorrect',\n",
    "    'MiddlePhalanxTW',\n",
    "    'MoteStrain',\n",
    "    'NonInvasiveFetalECGThorax1',\n",
    "    'NonInvasiveFetalECGThorax2',\n",
    "    'OliveOil',\n",
    "    'OSULeaf',\n",
    "    'PhalangesOutlinesCorrect',\n",
    "    'Phoneme',\n",
    "    'Plane',\n",
    "    'ProximalPhalanxOutlineAgeGroup',\n",
    "    'ProximalPhalanxOutlineCorrect',\n",
    "    'ProximalPhalanxTW',\n",
    "    'RefrigerationDevices',\n",
    "    'ScreenType',\n",
    "    'ShapeletSim',\n",
    "    'ShapesAll',\n",
    "    'SmallKitchenAppliances',\n",
    "    'SonyAIBORobotSurface1',\n",
    "    'SonyAIBORobotSurface2',\n",
    "    'StarLightCurves',\n",
    "    'Strawberry',\n",
    "    'SwedishLeaf',\n",
    "    'Symbols',\n",
    "    'SyntheticControl',\n",
    "    'ToeSegmentation1',\n",
    "    'ToeSegmentation2',\n",
    "    'Trace',\n",
    "    'TwoLeadECG',\n",
    "    'TwoPatterns',\n",
    "    'UWaveGestureLibraryAll',\n",
    "    'UWaveGestureLibraryX',\n",
    "    'UWaveGestureLibraryY',\n",
    "    'UWaveGestureLibraryZ',\n",
    "    'Wafer',\n",
    "    'Wine',\n",
    "    'WordSynonyms',\n",
    "    'Worms',\n",
    "    'WormsTwoClass',\n",
    "    'Yoga',\n",
    "    'ACSF1',\n",
    "    'BME',\n",
    "    'Chinatown',\n",
    "    'Crop',\n",
    "    'EOGHorizontalSignal',\n",
    "    'EOGVerticalSignal',\n",
    "    'EthanolLevel',\n",
    "    'FreezerRegularTrain',\n",
    "    'FreezerSmallTrain',\n",
    "    'Fungi',\n",
    "    'GunPointAgeSpan',\n",
    "    'GunPointMaleVersusFemale',\n",
    "    'GunPointOldVersusYoung',\n",
    "    'HouseTwenty',\n",
    "    'InsectEPGRegularTrain',\n",
    "    'InsectEPGSmallTrain',\n",
    "    'MixedShapesRegularTrain',\n",
    "    'MixedShapesSmallTrain',\n",
    "    'PigAirwayPressure',\n",
    "    'PigArtPressure',\n",
    "    'PigCVP',\n",
    "    'PowerCons',\n",
    "    'Rock',\n",
    "    'SemgHandGenderCh2',\n",
    "    'SemgHandMovementCh2',\n",
    "    'SemgHandSubjectCh2',\n",
    "    'SmoothSubspace',\n",
    "    'UMD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'dataset': [],\n",
    "    'model': [],\n",
    "    'label': [],\n",
    "    'accuracy': [],\n",
    "    'f1': [],\n",
    "    'recall': [],\n",
    "    'precision': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in UCR_DATASETS:\n",
    "    print(f'Starting experiments with {dataset} dataset...')\n",
    "    # Load the data from .tsv files\n",
    "    train_data = np.genfromtxt(f'../data/ucr/{dataset}/{dataset}_TRAIN.tsv')\n",
    "    x_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "    \n",
    "    test_data = np.genfromtxt(f'../data/ucr/{dataset}/{dataset}_TEST.tsv')\n",
    "    x_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "    \n",
    "    unique_labels = np.unique(y_train)\n",
    "    for label in unique_labels:\n",
    "        print(f'\\tClassifying the label {label}...')\n",
    "        # Filter samples from positive label\n",
    "        x_train_ = x_train[y_train == label]\n",
    "        y_train_ = y_train[y_train == label]\n",
    "\n",
    "        y_test_ = np.array([1 if y_true == label else -1 for y_true in y_test])\n",
    "        \n",
    "        # Apply z normalization\n",
    "        std_ = x_train_.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_train_ = (x_train_ - x_train_.mean(axis=1, keepdims=True)) / std_\n",
    "        \n",
    "        std_ = x_test.std(axis=1, keepdims=True)\n",
    "        std_[std_ == 0] = 1.0\n",
    "        x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n",
    "    \n",
    "        x_train_ = np.expand_dims(x_train_, axis=1)\n",
    "        x_test_ = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "        train_set = BaseDataset(x=x_train_, y=y_train_)\n",
    "        test_set = BaseDataset(x=x_test_, y=y_test_)\n",
    "        \n",
    "        train_loader = DataLoader(train_set, batch_size=16)\n",
    "        test_loader = DataLoader(test_set, batch_size=16)\n",
    "\n",
    "        # Train the autoencoder to learn the data representation over the new space\n",
    "        autoencoder = DeepSVDDAutoEncoder(x_train_.shape[-1], in_channels=1)\n",
    "        trainer = pl.Trainer(max_epochs=350, accelerator='gpu', devices=-1)\n",
    "        trainer.fit(autoencoder, train_dataloaders=train_loader)\n",
    "        \n",
    "        deepsvdd = DeepSVDD(sequence_length=x_train_.shape[-1], in_channels=1)\n",
    "        deepsvdd.load_state_dict(autoencoder.state_dict(), strict=False)\n",
    "        deepsvdd.to(torch.device('cuda'))\n",
    "\n",
    "        center = deepsvdd.init_center(train_loader)\n",
    "        deepsvdd.center = center\n",
    "\n",
    "        trainer_deepsvdd = pl.Trainer(max_epochs=350, accelerator='gpu', devices=-1)\n",
    "        trainer_deepsvdd.fit(deepsvdd, train_dataloaders=train_loader)\n",
    "        \n",
    "        metrics = trainer_deepsvdd.test(deepsvdd, dataloaders=test_loader)[0]\n",
    "        \n",
    "        results['dataset'].append(dataset)\n",
    "        results['model'].append('deepsvdd')\n",
    "        results['label'].append(label)\n",
    "        results['accuracy'].append(metrics['accuracy_score'])\n",
    "        results['f1'].append(metrics['f1'])\n",
    "        results['recall'].append(metrics['recall'])\n",
    "        results['precision'].append(metrics['precision'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
